---
layout: category
title: Publications
---

<details open="">
<summary><t-half><span><strong>Preprints</strong></span></t-half></summary>
<t1>
<ul style="background-color: #f2f2f2;">

<li><span><strong>Image desnowing via deep invertible separation</strong> </span><br />
<span> Y. Quan, X. Tan, Y. Huang, Y. Xu and H. Ji <br />
<em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), </em>xx(x): xxx–xxx, xxx xxx</span></li>

<li><span><strong>Self-supervised blind image deconvolution via deep generative ensemble learning</strong> [<a href="https://csyhquan.github.io/manuscript/22x-Self-Supervised%20Blind%20Image%20Deconvolution%20via%20Deep%20Generative%20Ensemble%20Learning.pdf">manuscript</a>]</span><br />
<span> M. Chen, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, Y. Xu and H. Ji <br />
<em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), </em>xx(x): xxx–xxx, xxx xxx</span></li>




</ul>
</t1>
</details>


<details open="">
<summary><t-half><span><strong>2022</strong></span></t-half></summary>
<t1>
<ul style="background-color: #f2f2f2;">

<li><span><strong>Unsupervised deep learning for phase retrieval via teacher-student distillation</strong> </span><br />
<span> Y. Quan, Z. Chen, T. Pang, and H. Ji <br />
<em>AAAI Conference on Artificial Intelligence (AAAI), </em>2023</span></li>


</ul>
</t1>
</details>

<details open="">
<summary><t-half><span><strong>2022</strong></span></t-half></summary>
<t1>
<ul style="background-color: #f2f2f2;">


<li><span><strong>Dual-domain self-supervised learning and model adaption for deep compressive imaging</strong> [<a href="https://csyhquan.github.io/manuscript/22-eccv-Dual-Domain%20Self-Supervised%20Learning%20and%20Model%20Adaption%20for%20Deep%20Compressive%20Imaging.pdf">manuscript</a>][<a href="https://github.com/XinranQin/DualDomainSS" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, X. Qin✉, T. Pang, and H. Ji <br />
<em>European Conference on Computer Vision (ECCV), </em>2022</span></li>

<li><span><strong>Learning deep non-blind image deconvolution without ground truths</strong> [<a href="https://csyhquan.github.io/manuscript/22-eccv-Learning%20Deep%20Non-Blind%20Image%20Deconvolution%20Without%20Ground%20Truths.pdf">manuscript</a>]</span><br />
<span> Y. Quan, Z. Chen✉, H. Zheng, and H. Ji <br />
<em>European Conference on Computer Vision (ECCV), </em>2022</span></li>

<li><span><strong>Self-supervised low-light image enhancement using discrepant untrained network priors</strong> [<a href="https://csyhquan.github.io/manuscript/22-tcsvt-Self-Supervised%20Low-Light%20Image%20Enhancement%20Using%20Discrepant%20Untrained%20Network%20Priors.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/22-tcsvt-Self-Supervised%20Low-Light%20Image%20Enhancement%20Using%20Discrepant%20Untrained%20Network%20Priors%20(SUPP).pdf">supp</a>][<a href="https://github.com/sherrycattt/discrepant-untrained-nn-priors" ><font color="#F75000">github</font></a>]</span><br />
<span> J. Liang, Y. Xu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, B. Shi, and H. Ji <br />
<em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), </em>32(11): 7332-7345, 2022</span></li>

<li><span><strong>Nonblind image deblurring via deep learning in complex field</strong> [<a href="https://csyhquan.github.io/manuscript/22-tnnls-Nonblind%20Image%20Deblurring%20via%20Deep%20Learning%20in%20Complex%20Field.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/22-tnnls-Nonblind%20Image%20Deblurring%20via%20Deep%20Learning%20in%20Complex%20Field%20(SUPP).pdf">supp</a>]</span><br />
<span> Y. Quan, P. Lin, Y. Xu✉, Y. Nan, and H. Ji <br />
<em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), </em>33(10): 5387-5400, 2022</span></li>

<li><span><strong>Unsupervised knowledge transfer for nonblind image deconvolution</strong>[<a href="https://csyhquan.github.io/manuscript/22-prl-Unsupervised%20Knowledge%20Transfer%20for%20Nonblind%20Image%20Deconvolution.pdf">manuscript</a>]</span><br />
<span> Z. Chen, X. Yao, Y. Xu, J. Wang, and Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a> <br />
<em>Pattern Recognition Letters, </em>164: 232–238, 2022</span></li>

<li><span><strong>A dataset-free deep learning method for low-dose CT image reconstruction</strong> [<a href="https://csyhquan.github.io/manuscript/22-ip-A%20Dataset-free%20Deep%20Learning%20Method%20for%20Low-Dose%20CT%20Image%20Reconstruction.pdf">manuscript</a>]</span><br />
<span> Q. Ding, H. Ji, Y. Quan, and X. Zhang <br />
<em>Inverse Problems (IP), </em>38, 104003, 2022</span></li>

<li><span><strong>No-reference image quality assessment using dynamic complex-valued neural model</strong> [<a href="https://csyhquan.github.io/manuscript/22-mm-No-Reference%20Image%20Quality%20Assessment%20Using%20Dynamic%20Complex-Valued%20Neural%20Model.pdf">manuscript</a>]</span><br />
<span> Z. Zhou, Y. Xu, R. Xu✉, and Y. Quan <br />
<em>ACM Multimedia (MM), </em>2022</span></li>

<li><span><strong>Deep blind image quality assessment using dual-order statistics</strong> [<a href="https://csyhquan.github.io/manuscript/22-icme-Deep%20Blind%20Image%20Quality%20Assessment%20Using%20Dual-Order%20Statistics.pdf">manuscript</a>]</span><br />
<span> Z. Zhou, Y. Xu, Y. Quan, and R. Xu✉ <br />
<em>IEEE International Conference on Multimedia and Expo (ICME), </em>2022</span></li>

<li><span><strong>Nonblind image deconvolution via leveraging model uncertainty in an untrained deep neural network</strong> [<a href="https://csyhquan.github.io/manuscript/22-Nonblind%20Image%20Deconvolution%20via%20Leveraging%20Model%20Uncertainty%20in%20An%20Untrained%20Deep%20Neural%20Network.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/22-Nonblind%20Image%20Deconvolution%20via%20Leveraging%20Model%20Uncertainty%20in%20An%20Untrained%20Deep%20Neural%20Network%20(SUPP).pdf">supp</a>][<a href="https://github.com/scut-mingqinchen/Model_Uncertainty_NID" ><font color="#F75000">github</font></a>]</span><br />
<span> M. Chen, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, T. Pang, and H. Ji <br />
<em>International Journal of Computer Vision (IJCV), </em>130: 1770–1789, 2022</span></li>


<li><span><strong>Unsupervised phase retrieval using deep approximate MMSE estimation</strong> [<a href="https://csyhquan.github.io/manuscript/22-tsp-Unsupervised%20Phase%20Retrieval%20Using%20Deep%20Approximate%20MMSE%20Estimation.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/22-tsp-Unsupervised%20Phase%20Retrieval%20Using%20Deep%20Approximate%20MMSE%20Estimation%20(SUPP).pdf">supp</a>][<a href="https://github.com/AlanLin1995/DeepMMSE" ><font color="#F75000">github</font></a>]</span><br />
<span> M. Chen, P. Lin, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, T. Pang, and H. Ji <br />
<em>IEEE Transactions on Signal Processing (TSP), </em>70: 2239–2252, 2022</span></li>


<li><span><strong>Unsupervised deep background matting using deep matte prior</strong> [<a href="https://csyhquan.github.io/manuscript/22-tcsvt-Unsupervised%20Deep%20Background%20Matting%20Using%20Deep%20Matte%20Prior.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/22-tcsvt-Unsupervised%20Deep%20Background%20Matting%20Using%20Deep%20Matte%20Prior%20(SUPP).pdf">supp</a>][<a href="https://github.com/lbl680/deep-matte-prior" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Xu, B. Liu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and H. Ji <br />
<em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), </em>32(7): 4324–4337, 2022</span></li>


<li><span><strong>High-quality self-supervised snapshot hyperspectral imaging</strong> [<a href="https://csyhquan.github.io/manuscript/22-ICASSP-HSI.pdf">manuscript</a>][<a href="https://github.com/XinranQin/HQSCI/" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, X. Qin, M. Chen, and Y. Huang✉ <br />
<em>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), </em>2022</span></li>

<li><span><strong>Recurrent exposure generation for low-light face detection</strong> [<a href="https://csyhquan.github.io/manuscript/22-tmm-Recurrent%20Exposure%20Generation%20for%20Low-Light%20Face%20Detection.pdf">manuscript</a>][<a href="https://github.com/sherrycattt/REGDet" ><font color="#F75000">github</font></a>]</span><br />
<span> J. Liang, J. Wang, Y. Quan, T. Chen, J. Liu, H. Ling, and Y. Xu✉ <br />
<em>IEEE Transactions on Multimedia (TMM), </em>24: 1609–1621, 2022</span></li>


</ul>
</t1>
</details>

<details open="">
<summary><t-half><span><strong>2021</strong></span></t-half></summary>
<t1>
<ul style="background-color: #f2f2f2;">



<li><span><strong>Gaussian kernel mixture network for single image defocus deblurring</strong> [<a href="https://csyhquan.github.io/manuscript/21-nips-Gaussian%20kernel%20mixture%20network%20for%20single%20image%20defocus%20deblurring.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/21-nips-Gaussian%20kernel%20mixture%20network%20for%20single%20image%20defocus%20deblurring%20(SUPP).pdf">supp</a>][<a href="https://github.com/csZcWu/GKMNet" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, Z. Wu, and H. Ji <br />
<em>Annual Conference on Neural Information Processing Systems (NeurIPS), </em>2021</span></li>



<li><span><strong>Encoding spatial distribution of convolutional features for texture representation</strong> [<a href="https://csyhquan.github.io/manuscript/21-nips-Encoding%20spatial%20distribution%20of%20convolutional%20features%20for%20texture%20representation.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/21-nips-Encoding%20spatial%20distribution%20of%20convolutional%20features%20for%20texture%20representation%20(SUPP).pdf">supp</a>][<a href="https://github.com/csfengli/FENet" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Xu, F. Li, Z. Chen, J. Liang, and Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a> <br />
<em>Annual Conference on Neural Information Processing Systems (NeurIPS), </em>2021</span></li>



<li><span><strong>Deep texture recognition via exploiting cross-layer statistical self-similarity</strong> [<a href="https://csyhquan.github.io/manuscript/21-cvpr-Deep%20Texture%20Recognition%20via%20Exploiting%20Cross-Layer%20Statistical%20Self-Similarity.pdf">manuscript</a>]</span><br />
<span> Z. Chen, F. Li, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, Y. Xu, and H. Ji <br />
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2021</span></li>



<li><span><strong>Recorrupted-to-Recorrupted: Unsupervised deep learning for image denoising</strong> [<a href="https://csyhquan.github.io/manuscript/21-cvpr-Recorrupted-to-Recorrupted%20Unsupervised%20Deep%20Learning%20for%20Image%20Denoising.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/21-cvpr-Recorrupted-to-Recorrupted%20Unsupervised%20Deep%20Learning%20for%20Image%20Denoising%20(SUPP).pdf">supp</a>][<a href="https://github.com/PangTongyao/Recorrupted-to-Recorrupted-Unsupervised-Deep-Learning-for-Image-Denoising" ><font color="#F75000">github</font></a>]</span><br />
<span> T. Pang, H. Zheng, Y. Quan, and H. Ji <br />
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2021</span></li>



<li><span><strong>Multi-view 3D shape recognition via correspondence-aware deep learning</strong> [<a href="https://csyhquan.github.io/manuscript/21-tip-Multi-View%203D%20Shape%20Recognition%20via%20Correspondence-Aware%20Deep%20Learning.pdf
">manuscript</a>]</span><br />
<span> Y. Xu, C. Zheng, R. Xu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and H. Ling <br />
<em>IEEE Transactions on Image Processing (TIP), </em>30: 5299–5312, 2021</span></li>

<li><span><strong>Image quality assessment using kernel sparse coding</strong> [<a href="https://csyhquan.github.io/manuscript/21-tmm-Image%20Quality%20Assessment%20Using%20Kernel%20Sparse%20Coding.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/21-tmm-Image%20Quality%20Assessment%20Using%20Kernel%20Sparse%20Coding%20(SUPP).pdf">supp</a>][<a href="https://csyhquan.github.io/code/TMM-kernel/TMM-KSC_IQA-FinalCode.rar"><font color="#F75000">code</font></a>][<a href="https://github.com/JoanneZZH/KSC-IQA" ><font color="#F75000">github</font></a>] </span><br />
<span> Z. Zhou, J. Li, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and R. Xu <br />
<em>IEEE Transactions on Multimedia (TMM), </em>23: 1592–1604, 2021</span></li>

<li><span><strong>Watermarking deep neural networks in image processing</strong>  [<a href="https://csyhquan.github.io/manuscript/21-tnnls-Watermarking%20Deep%20Neural%20Networks%20in%20Image%20Processing.pdf">manuscript</a>][<a href="https://github.com/painfulloop/Watermark-DnCNN.git" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, H. Teng, Y. Chen, and H. Ji✉ <br />
<em>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), </em>32(5): 1852–1865, 2021</span></li>


<li><span><strong>Factorized tensor dictionary learning for visual tensor data completion</strong>  [<a href="https://csyhquan.github.io/manuscript/21-tmm-Factorized%20Tensor%20Dictionary%20Learning%20for%20Visual%20Tensor%20Data%20Completion.PDF">manuscript</a>][<a href="https://csyhquan.github.io/code/tensor.rar"><font color="#F75000">code</font></a>]</span><br />
<span>R. Xu, Y. Xu, and Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a> <br />
<em>IEEE Transactions on Multimedia (TMM), </em>23: 1225–1238, 2021</span></li>


<li><span><strong>Attentive deep network for blind motion deblurring on dynamic scenes</strong> [<a href="https://csyhquan.github.io/manuscript/21-cviu-Attentive%20Deep%20Network%20for%20Blind%20Motion%20Deblurring%20on%20Dynamic%20Scenes.pdf">manuscript</a>][<a href="https://github.com/zhuyeye/Attentive-Deblurring"><font color="#F75000">github</font></a>]</span><br />
<span> Y. Xu, Y. Zhu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and H. Ji <br />
<em>Computer Vision and Image Understanding (CVIU), </em>205, 2021</span></li>


<li><span><strong>Image denoising using complex-valued deep CNN</strong> [<a href="https://csyhquan.github.io/manuscript/21-pr-Image%20Denoising%20Using%20Complex-Valued%20Deep%20CNN.pdf">manuscript</a>][<a href="https://github.com/AlanLin1995/ComplexNet_denoise" ><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, Y. Chen, Y. Shao, H. Teng, Y. Xu✉, and H. Ji <br />
<em>Pattern Recognition (PR), </em>111, 2021</span></li>


<li><span><strong>Structure-texture image decomposition using discriminative patch recurrence</strong> [<a href="https://csyhquan.github.io/manuscript/21-tip-Structure-Texture%20Image%20Decomposition%20Using%20Discriminative%20Patch%20Recurrence.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/21-tip-Structure-Texture%20Image%20Decomposition%20Using%20Discriminative%20Patch%20Recurrence%20(SUPP).pdf">supp</a>][<a href="https://csyhquan.github.io/code/isotropy.rar" ><font color="#F75000">code</font></a>][<a href="https://github.com/RuotaoXu/IsoDecompose" ><font color="#F75000">github</font></a>]</span><br />
<span> R. Xu, Y. Xu, and Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a> <br />
<em>IEEE Transactions on Image Processing (TIP), </em>30: 1542-1555, 2021</span></li>


</ul>
</t1>
</details>


<details open="">
<summary><span><strong><t-half>2019 - 2020</t-half></strong></span></summary>
<t1>
<ul style="background-color: #f2f2f2;">

<li><span><strong>Self-supervised Bayesian deep learning for image recovery with applications to compressed sensing</strong>  [<a href="https://csyhquan.github.io/manuscript/20-eccv-Self-supervised%20Bayesian%20Deep%20Learning%20for%20Image%20Recovery%20with%20Applications%20to%20Compressive%20Sensing.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/20-eccv-Self-supervised%20Bayesian%20Deep%20Learning%20for%20Image%20Recovery%20with%20Applications%20to%20Compressive%20Sensing%20(SUPP).pdf">supp</a>][<a href="https://github.com/PangTongyao/Self-supervised-BNN-for-image-recovery" ><font color="#F75000">github</font></a>]</span><br />
<span> T. Pang, Y. Quan, and H. Ji<br />
<em>European Conference on Computer Vision (ECCV), </em>2020</span></li><!--(virtual），Aug-->

 <li><span><strong>Full-reference image quality metric for blurry images and compressed images using hybrid dictionary learning</strong> [<a href="https://csyhquan.github.io/manuscript/20-nca-Full-reference%20image%20quality%20metric%20for%20blurry%20images%20and%20compressed%20images%20using%20hybrid%20dictionary%20learning.pdf">manuscript</a>][<a href="https://github.com/JoanneZZH/HDL-IQA/" ><font color="#F75000">github</font></a>] </span><br />
<span> Z. Zhou, J. Li, Y. Xu, and Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a> <br />
<em> Neural Computing and Applications (NCA), </em>32(16): 12403-12415, 2020</span></li>

<li><span><strong>Cartoon-texture image decomposition using orientation characteristics in patch recurrence</strong>  [<a href="https://csyhquan.github.io/manuscript/21-siam-Cartoon-Texture%20Image%20Decomposition%20using%20Orientation%20Characteristics%20in%20Patch%20Recurrence.pdf">manuscript</a>] 
</span><br />
<span> R. Xu, Y. Xu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and H. Ji <br />
<em>SIAM Journal on Imaging Sciences (SIIMS), </em>13(3): 1179–1210, 2020</span></li>


<li><span><strong>Self2Self with dropout: Learning self-supervised denoising from single image</strong>  [<a href="https://csyhquan.github.io/manuscript/20-cvpr-Self2Self%20With%20Dropout%20Learning%20Self-Supervised%20Denoising%20From%20Single%20Image.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/20-cvpr-Self2Self%20With%20Dropout%20Learning%20Self-Supervised%20Denoising%20From%20Single%20Image%20(SUPP).pdf">supp</a>][<a href="https://csyhquan.github.io/code/Self2Self/Self2Self.rar"><font color="#F75000">code</font></a>][<a href="https://github.com/scut-mingqinchen/self2self"><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, M. Chen, T. Pang, and H. Ji<br />
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2020</span></li><!--Seattle(virtual），Jun-->


<li><span><strong>Variational-EM-based deep learning for noise-blind image deblurring</strong>  [<a href="https://csyhquan.github.io/manuscript/20-cvpr-Variational-EM-based%20Deep%20Learning%20for%20Noise-blind%20Image%20Deblurring.pdf">manuscript</a>]</span><br />
<span> Y. Nan, Y. Quan, and H. Ji <br />
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2020</span></li><!--Seattle(virtual），Jun-->


<li><span><strong>Collaborative deep learning for super-resolving blurry text images</strong>  [<a href="https://csyhquan.github.io/manuscript/20-tci-Collaborative%20Deep%20Learning%20for%20Super-Resolving%20Blurry%20Text%20Images.pdf">manuscript</a>][<a href="https://github.com/csjietingyang/ImplementationOfOurAcceptedPaper"><font color="#F75000">github</font></a>]</span><br />
<span>Y. Quan, J. Yang, Y. Chen, Y. Xu✉, and H. Ji <br />
<em>IEEE Transactions on Computational Imaging (TCI), </em>6: 778-790, 2020</span></li>


<li><span><strong>Image denoising via sequential ensemble learning</strong> [<a href="https://csyhquan.github.io/manuscript/20-tip-Image%20Denoising%20via%20Sequential%20Ensemble%20Learning.pdf">manuscript</a>][<a href="https://github.com/cs-rukawa/NLED_Code"><font color="#F75000">github</font></a>]</span><br />
<span> X. Yang, Y. Xu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and H. Ji<br />
<em> IEEE Transactions on Image Processing (TIP), </em>29: 5038-5049, 2020</span></li>



<li><span><strong>Weakly-supervised sparse coding with geometric prior for interactive texture segmentation</strong> [<a href="https://csyhquan.github.io/manuscript/20-spl-Weakly-Supervised%20Sparse%20Coding%20with%20Geometric%20Prior%20for%20Interactive%20Texture%20Segmentation.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/20-spl-Weakly-Supervised%20Sparse%20Coding%20with%20Geometric%20Prior%20for%20Interactive%20Texture%20Segmentation%20(SUPP).pdf">supp</a>][<a href="https://github.com/csyanhuang/texSeg"><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, H. Teng, T. Liu, and Y. Huang✉ <br />
<em> IEEE Signal Processing Letters (SPL), </em>27: 116-120, 2020</span></li> 

<li><span><strong>Removing reflection from a single image with ghosting effect</strong> [<a href="https://csyhquan.github.io/manuscript/20-tci-Removing%20Reflection%20From%20a%20Single%20Image%20With%20Ghosting%20Effect.pdf">manuscript</a>]</span><br />
<span> Y. Huang, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, Y. Xu, R. Xu, and H. Ji <br />
<em> IEEE Transactions on Computational Imaging (TCI), </em>6: 34-45, 2020</span></li>



</ul>
</t1>
</details>


<details open="">
<summary><span><strong><t-half>2019 - 2020</t-half></strong></span></summary>
<t1>
<ul style="background-color: #f2f2f2;">


 <li><span><strong>Barzilai-Borwein-based adaptive learning rate for deep learning</strong> [<a href="https://csyhquan.github.io/manuscript/19-pr-Barzilai%E2%80%93Borwein-based%20adaptive%20learning%20rate%20for%20deep%20learning.pdf">manuscript</a>][<a href="https://github.com/sherrycattt/bb_dl.pytorch"><font color="#F75000">github</font></a>]</span><br />
<span> J. Liang, Y. Xu, C. Bao, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and H. Ji <br />
 <em> Pattern Recognition Letters (PRL), </em>128: 197-203, 2019</span></li>

 <li><span><strong>Exploiting global low-rank structure and local sparsity nature for tensor completion</strong> [<a href="https://csyhquan.github.io/manuscript/19-tcyb-Exploiting%20Global%20Low-rank%20Structure%20and%20Local%20Sparsity%20Nature%20for%20Tensor%20Completion.pdf">manuscript</a>][<a href="https://github.com/csyongdu/Exploiting-Global-Low-Rank-Structure-and-Local-Sparsity-Nature-for-Tensor-Completion"><font color="#F75000">github</font></a>] </span><br />
<span> Y. Du, G. Han✉, Y. Quan, Z. Yu, H. Wong, C. Chen, and J. Zhang <br />
<em> IEEE Transactions on Cybernetics (TCYB),</em> 49(11): 3898-3910, 2019</span></li>


<li><span><strong>Deep learning for seeing through window with raindrops</strong> [<a href="https://csyhquan.github.io/manuscript/19-iccv-Deep%20Learning%20for%20Seeing%20Through%20Window%20With%20Raindrops.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/19-iccv-Deep%20Learning%20for%20Seeing%20Through%20Window%20With%20Raindrops%20(SUPP).pdf">supp</a>][<a href="https://github.com/jackiesdd/raindropAttention"><font color="#F75000">github</font></a>]</span><br />
<span> Y. Quan, S. Deng, Y. Chen, and H. Ji<br />
 <em> IEEE International Conference on Computer Vision (ICCV), </em>2019</span></li><!--Seoul, Oct -->
 <!--
<li><span style="font-size: 95%;"><strong>Exploiting label consistency in structured sparse representation for classification</strong> [<a href="https://csyhquan.github.io/manuscript/19-nca-Exploiting%20label%20consistency%20in%20structured%20sparse%20representation%20for%20classification.pdf">manuscript</a>]</span><br />
<span style="font-size: 95%;"> Y. Huang, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, T. Liu and Y. Xu,<br />
<em> Neural Computing and Applications (NCA), </em>31(10): 6509-6520, Oct 2019</span></li>-->
<li><span><strong>Attention with structure regularization for action recognition</strong> [<a href="https://csyhquan.github.io/manuscript/19-cviu-Attention%20with%20structure%20regularization%20for%20action%20recognition.pdf">manuscript</a>]</span><br />
<span> Y. Quan, Y. Chen, R. Xu✉, and H. Ji <br />
<em> Computer Vision and Image Understanding (CVIU), </em>187, 2019</span></li>
<!--
<li><span style="font-size: 95%;"><strong>Deeply exploiting long-term view dependency for 3D shape recognition</strong> [<a href="https://csyhquan.github.io/manuscript/19-access-Deeply%20Exploiting%20Long-Term%20View%20Dependency%20for%203D%20Shape%20Recognition.pdf">manuscript</a>] </span><br />
<span style="font-size: 95%;"> Y. Xu, C. Zheng, R. Xu and Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>,<br />
<em> IEEE Access (ACCESS), </em>7: 111678-111691, Aug 2019</span></li>-->

 <li><span><strong>Supervised sparse coding with decision forest</strong> [<a href="https://csyhquan.github.io/manuscript/19-spl-Supervised%20Sparse%20Coding%20With%20Decision%20Forest.pdf">manuscript</a>][<a href="https://github.com/csyanhuang/SCDF" ><font color="#F75000">github</font></a>] </span><br />
<span> Y. Huang, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, and T. Liu<br />
 <em> IEEE Signal Processing Letters (SPL), </em>26(2): 327-331, 2019</span></li>
</ul>
</t1>
</details>



<details>
<summary><span><strong><t-half>2017 - 2018</t-half></strong></span></summary>
<t1>
<ul style="background-color: #f2f2f2;">
<!--
<li><span style="font-size: 95%;"><strong>Sparse coding and dictionary learning with class-speciﬁc group sparsity</strong> [<a href="https://github.com/csyhquan/csyhquan.github.io/raw/master/manuscript/18-nca-Sparse%20coding%20and%20dictionary%20learning%20with%20class-speci%EF%AC%81c%20group%20sparsity.pdf">manuscript</a>] </span><br />
<span style="font-size: 95%;"> Y. Sun, Y. Quan and J. Fu,<br />
<em> Neural Computing and Applications (NCA),</em> 30(4): 1265-1275, Aug 2018</span></li>-->


<li><span><strong>Image-based action recognition using hint-enhanced deep neural network</strong> [<a href="https://csyhquan.github.io/manuscript/17-nc-Image-based%20action%20recognition%20using%20hint-enhanced%20deep%20neural%20network.pdf">manuscript</a>] </span><br />
<span> T. Qi, Y. Xu✉, Y. Quan, Y. Wang, and H. Ling<br />
<em>Neurocomputing (NC), </em>267: 475-488, 2017</span></li>
<li><span><strong>Spatiotemporal lacunarity spectrum for dynamic texture classification</strong> [<a href="https://csyhquan.github.io/manuscript/17-cviu-Spatiotemporal%20lacunarity%20spectrum%20for%20dynamic%20texture%20classification.pdf">manuscript</a>] </span><br />
<span> Y. Quan, Y. Sun✉, and Y. Xu <br />
<em>Computer Vision and Image Understanding (CVIU), </em>165: 85-96, 2017</span></li>
<li><span><strong>Estimating defocus blur via rank of local patches</strong> [<a href="https://csyhquan.github.io/manuscript/17-iccv-Estimating%20Defocus%20Blur%20via%20Rank%20of%20Local%20Patches.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/17-iccv-Estimating%20Defocus%20Blur%20via%20Rank%20of%20Local%20Patches%20(SUPP).pdf">supp</a>][<a href="https://csyhquan.github.io/code/17-iccv-Estimating%20Defocus%20Blur%20via%20Rank%20of%20Local%20Patches/Defocus_estimator_v1.0.rar"><font color="#F75000">code</font></a>]</span><br />
<span> G. Xu, Y. Quan, and H. Ji<br />
<em> IEEE International Conference on Computer Vision (ICCV), </em>2017</span></li><!--Venice, Oct -->
</ul>
</t1>
</details>




<details>
<summary><span><strong><t-half>2015 - 2016</t-half></strong></span></summary>
<t1>
<ul style="background-color: #f2f2f2;">
<li><span><strong>Dictionary learning for sparse coding: Algorithms and convergence analysis</strong> [<a href="https://csyhquan.github.io/manuscript/16-tpami-Dictionary%20learning%20for%20sparse%20coding_Algorithms%20and%20convergence%20analysis.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/l0dl_int.rar" ><font color="#F75000">code</font></a>] </span><br />
<span> C. Bao, H. Ji✉, Y. Quan, and Z. Shen<br />
<em> IEEE Transactions on Patter Analysis and Machine Intelligence (TPAMI),</em> 38(7): 1356-1369, 2016</span></li>
<li><span><strong>Supervised dictionary learning with multiple classifier integration</strong> [<a href="https://csyhquan.github.io/manuscript/16-pr-Supervised%20dictionary%20learning%20with%20multiple%20classifier%20integration.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/16-pr-Supervised%20dictionary%20learning%20with%20multiple%20classifier%20integration/MCDLv7_pcode.rar"><font color="#F75000">code</font></a>] </span><br />
<span>Y. Quan, Y. Xu✉, Y. Sun, and Y. Huang <br />
<em> Pattern Recognition (PR),</em> 55: 247-260, 2016</span></li>

<li><span><strong>Equiangular kernel dictionary learning with applications to dynamic texture analysis</strong> [<a href="https://csyhquan.github.io/manuscript/16-cvpr-Equiangular%20Kernel%20Dictionary%20Learning%20with%20Applications%20to%20Dynamic%20Texture%20Analysis.pdf">manuscript</a>][<a href="https://csyhquan.github.io/manuscript/16-cvpr-Equiangular%20Kernel%20Dictionary%20Learning%20with%20Applications%20to%20Dynamic%20Texture%20Analysis%20(SUPP).pdf">supp</a>]</span><br />
<span>Y. Quan, C. Bao, and H. Ji<br />
<em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2016</span></li><!--Las Vegas, Jun, -->

<li><span><strong>Sparse coding for classification via discrimination ensemble</strong> [<a href="https://csyhquan.github.io/manuscript/16-cvpr-Sparse%20Coding%20for%20Classification%20via%20Discrimination%20Ensemble.pdf">manuscript</a>]</span><br />
<span>Y. Quan, Y. Xu, Y. Sun, Y. Huang, and H. Ji <br />
<em> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2016</span></li> <!--Las Vegas, Jun, -->


<li><span><strong>Dynamic texture recognition via orthogonal tensor dictionary learning</strong> [<a href="https://csyhquan.github.io/manuscript/15-iccv-Dynamic%20Texture%20Recognition%20via%20Orthogonal%20Tensor%20Dictionary%20Learning.pdf">manuscript</a>]</span><br />
<span>Y. Quan, Y. Huang, and H. Ji <br />

<em> IEEE International Conference on Computer Vision (ICCV), </em>2015</span></li><!--Santiago, Dec, -->
<li><span><strong>Classifying dynamic textures via spatiotemporal fractal analysis</strong> [<a href="https://csyhquan.github.io/manuscript/15-pr-Classifying%20dynamic%20textures%20via%20spatiotemporal%20fractal%20analysis.pdf">manuscript</a>] </span><br />
<span>Y. Xu, Y. Quan<a href="mailto:csyhquan@scut.edu.cn">✉</a>, Z. Zhang, H. Ling, and H. Ji<br />
<em> Pattern Recognition (PR),</em> 48(10): 3239-3248, 2015</span></li>

<li><span><strong>Structured sparse coding for classification via reweighted l<sub>2,1</sub> minimization</strong> [<a href="https://csyhquan.github.io/manuscript/15-cccv-Structured%20Sparse%20Coding%20for%20Classification%20via%20Reweighted%20l12%20minimization.pdf">manuscript</a>]</span><br />
<span>Y. Xu, Y. Sun, Y. Quan, and Y. Luo <br />
<em> The Chinese Conference on Computer Vision (CCCV), </em>2015</span></li><!--Xi'an, Sep, -->

<li><span><strong>Fractal analysis for reduced reference image quality assessment</strong> [<a href="https://csyhquan.github.io/manuscript/15-tip-Fractal%20Analysis%20for%20Reduced%20Reference%20Image%20Quality%20Assessment.pdf">manuscript</a>] </span><br />
<span> Y. Xu✉, D. Liu, Y. Quan, and P. Callet <br />
<em> IEEE Transactions on Image Processing (TIP),</em> 24(7): 2089-2109, 2015</span></li>

<li><span><strong>Discriminative structured dictionary learning with hierarchical group sparsity</strong> [<a href="https://csyhquan.github.io/manuscript/15-cviu-Discriminative%20structured%20dictionary%20learning%20with%20hierarchical%20group%20sparsity.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/15-cviu-Discriminative%20structured%20dictionary%20learning%20with%20hierarchical%20group%20sparsity/child_dl_v2.rar"><font color="#F75000">code</font></a>]</span><br />
<span>Y. Xu✉, Y. Sun, Y. Quan, and B. Zheng <br />
<em> Computer Vision and Image Understanding (CVIU),</em> 136: 59-68, 2015</span></li>

<li><span><strong>Characterizing dynamic textures with space-time lacunarity analysis</strong> [<a href="https://csyhquan.github.io/manuscript/15-icme-CHARACTERIZING%20DYNAMIC%20TEXTURES%20WITH%20SPACE-TIME%20LACUNARITY%20ANALYSIS.pdf">manuscript</a>]</span><br />
<span>Y. Sun, Y. Xu, and Y. Quan <br />
<em> IEEE International Conference on Multimedia and Expo (ICME), </em>Oral, 2015</span></li><!--Torino, Jun, -->

<li><span><strong>Directional regularity for visual quality estimation</strong> [<a href="https://csyhquan.github.io/manuscript/15-sp-Directional%20regularity%20for%20visual%20quality%20estimation.pdf">manuscript</a>] </span><br />
<span>D. Liu, Y. Xu✉, Y. Quan, Z. Yu, and P. Callet<br />
<em> Signal Processing (SP),</em> 110: 211-221, 2015</span></li>
<li><span><strong>Data-driven multi-scale non-local wavelet frame construction and image recovery</strong> [<a href="https://csyhquan.github.io/manuscript/15-josc-Data-driven%20multi-scale%20non-local%20wavelet%20frame%20construction%20and%20image%20recovery.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/15-josc-Data-driven%20multi-scale%20non-local%20wavelet%20frame%20construction%20and%20image%20recovery/NLFrame.rar"><font color="#F75000">code</font></a>] </span><br />
<span>Y. Quan, H. Ji✉, and Z. Shen <br />
<em> Journal of Scientific Computing (JoSC),</em> 63(2): 307-329, 2015</span></li>
</ul>
</t1>
</details>





<details>
<summary><span><strong><t-half>2013 - 2014</t-half></strong></span></summary>
<t1>
<ul style="background-color: #f2f2f2;">

<li><span><strong>A convergent incoherent dictionary learning algorithm for sparse coding</strong> [<a href="https://csyhquan.github.io/manuscript/14-eccv-A%20Convergent%20Incoherent%20Dictionary%20Learning%20Algorithm%20for%20Sparse%20Coding.pdf">manuscript</a>]</span><br />
<span>C. Bao, Y. Quan, and H. Ji<br />
<em> European Conference on Computer Vision (ECCV), </em>2014</span></li><!--Zurich, Sep -->

<li><span><strong>Reduced reference image quality assessment using regularity of phase congruency</strong> [<a href="https://csyhquan.github.io/manuscript/14-spic-Reduced%20Reference%20Image%20Quality%20Assessment%20Using%20Regularity%20of%20Phase%20Congruency.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/kernel-iqa.rar" ><font color="#F75000">code</font></a>] </span><br />
<span>D. Liu, Y. Xu✉, Y. Quan, and P. Callet<br />
<em> Signal Processing: Image Communication (SPIC),</em> 29(8): 844-855, 2014</span></li>

<li><span><strong>L<sub>0</sub> norm based dictionary learning by proximal methods with global convergence</strong> [<a href="https://csyhquan.github.io/manuscript/14-cvpr-l0%20norm%20based%20dictionary%20learning%20by%20proximal%20methods%20with%20global%20convergence.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/14-cvpr-l0%20norm%20based%20dictionary%20learning%20by%20proximal%20methods%20with%20global%20convergence/l0_dicti_learning_v2.rar"><font color="#F75000">code</font></a>]</span><br />
<span>C. Bao, H. Ji, Y. Quan, and Z. Shen <br />
<em>  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>Oral, 2014</span></li><!--Columbus, Jun -->

<li><span><strong>Lacunarity analysis on image patterns for texture classification</strong> [<a href="https://csyhquan.github.io/manuscript/14-cvpr-Lacunarity%20Analysis%20on%20Image%20Patterns%20for%20Texture%20Classification.pdf">manuscript</a>]</span><br />
<span>Y. Quan, Y. Xu, Y. Sun, and Y. Luo<br />
<em>  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2014</span></li><!--Columbus, Jun -->

<li><span><strong>A distinct and compact texture descriptor</strong> [<a href="https://csyhquan.github.io/manuscript/14-ivc-A%20distinct%20and%20compact%20texture%20descriptor.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/14-ivc-A%20distinct%20and%20compact%20texture%20descriptor/pfs_v1.rar"><font color="#F75000">code</font></a>]</span><br />
<span> Y. Quan, Y. Xu, and Y. Sun <br />
<em> Image and Vision Computing (IVC),</em> 32(4): 250-259, 2014</span></li>
</ul>
</t1>
</details>


<details>
<summary><span><strong><t-half>2011 - 2012</t-half></strong></span></summary>
<t1>
<ul style="background-color: #f2f2f2;">
<li><span><strong>Contour-based recognition</strong> [<a href="https://csyhquan.github.io/manuscript/12-cvpr-Contour-Based%20Recognition.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/12-cvpr-Contour-Based%20Recognition/mtp_demo_v3.rar"><font color="#F75000">code</font></a>]</span><br />
<span>Y. Xu, Y. Quan, Z. Zhang, H. Ji, C. Fermüller M. Nishigaki, and D. Dementhon <br />
<em>  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </em>2012</span></li><!--Rhode Island, Jun -->

<li><span ><strong>Dynamic texture classification using dynamic fractal analysis</strong> [<a href="https://csyhquan.github.io/manuscript/11-iccv-Dynamic%20Texture%20Classification%20Using%20Dynamic%20Fractal%20Analysis.pdf">manuscript</a>][<a href="https://csyhquan.github.io/code/11-iccv-Dynamic%20Texture%20Classification%20Using%20Dynamic%20Fractal%20Analysis/dfs_toolbox_v5.rar"><font color="#F75000">code</font></a>]</span><br />
<span>Y. Xu, Y. Quan, H. Ling, and H. Ji <br />
<em>  IEEE International Conference on Computer Vision (ICCV), </em>2011</span></li><!--Barcelona, Nov -->
</ul>
</t1>
</details>